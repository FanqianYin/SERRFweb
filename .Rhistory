colnames(newdata) =   c(paste0("X",1:nrow(eData))[-j], "batch", "time")
new = (eData[j,]/predict(model,newdata = newdata))*median(eData[j,])
return(new)
}, e.,batch,randomForest, QC.index, p[[time]])
e_SERRF_pred = t(pred)
# put the QC level bach to where they were.
for(i in 1:nrow(e_SERRF_pred)){
e_SERRF_pred[i,qc] = e_SERRF_pred[i,qc] + diff[i]
}
return(list(e = e_SERRF_pred, p = p, f = f))
}
norm = SERRF_norm(e, f, p, batch, QC.index, time = "time")
RSD = function(e,f,p,robust = F,cl){
if(robust){
result=parSapply(cl=cl,X=1:nrow(e),FUN = function(i,remove_outlier,e){
x = remove_outlier(e[i,])[[1]]
sd(x,na.rm=T)/mean(x,na.rm=T)
},remove_outlier,e)
}else{
result=parSapply(cl=cl,X=1:nrow(e),FUN = function(i,e){
x = e[i,]
sd(x,na.rm=T)/mean(x,na.rm=T)
},e)
}
return(result)
}
SERRF.validate = RSD(norm$e[,p$type=="validate"],f,p[p$type=="validate",],cl=cl)
raw.validate = RSD(e[,p$type=="validate"],f,p[p$type=="validate",],cl=cl)
stopCluster(cl)
# PCA
# generate PCA plot.
generate_PCA = function(e, f, p, QC.index, batch, method){
for(i in 1:nrow(e)){
e[i,is.na(e[i,])] = min(e[i,!is.na(e[i,])])
}
sds = apply(e,1,sd,na.rm = T)
sd_pos = sds>0
pca = prcomp(t(e[sd_pos,]), center = T, scale. = T)
variance = pca$sdev^2/sum(pca$sdev^2)
pca.data = data.frame(pca$x,batch = batch[1,],order = 1:nrow(pca$x))
batch.QC = batch[1,];
batch.QC[QC.index] = "QC"
qc = rep(F, nrow(p))
qc[QC.index] = TRUE
ggplot(pca.data, aes(PC1, PC2, color = batch.QC,size = qc, order = order)) +
geom_point(alpha = 3/4) +
stat_ellipse( linetype = 2, size = 0.5) +
labs(x = paste0("PC1: ",signif(variance[1]*100,3),"%"), y = paste0("PC2: ",signif(variance[2]*100,3),"%"),
title = method)+
theme.scatter
}
SERRFpca = generate_PCA(norm$e,f,p,QC.index, batch , "SERRF")
SERRFpca
rawpca = generate_PCA(e,f,p,QC.index, batch , "raw")
rawpca
workingdirectory = "C:\\Users\\Sili Fan\\Desktop\\WORK\\WCMC\\projects\\mx 370132 Anne"
workingdirectory = "C:\\Users\\Sili Fan\\Desktop\\WORK\\WCMC\\projects\\mx 370132 Anne"
# filename is the name of your file. Check out the example.xlsx for format requirements.
filename = "mx 370132 Anne Flennicken, mouse plasma, September 2017.xlsx"
options(warn=-1)
path = paste0(workingdirectory,"\\",filename)
if(!"pacman" %in% rownames(installed.packages())){
install.packages("pacman")
}
cat("Checking required packages (auto-installing if missing).\n")
pacman::p_load("randomForest", "affy", "e1071", "data.table", "parallel", "ReporteRs", "xlsx")
setwd(workingdirectory)
# load sources
source("normalizations.R")
source("utils.R")
source("evaluationMethods.R")
# read data.
cat("Reading Data.\n")
# metaData = read.csv("P20 dataset1 2016_11-30.csv")
# p = fread("p-pos.csv")
# p <- merge(p,metaData,by.x="Subject ID", by.y = "GBID", all.x = T,sort = F)
# f = fread("f-pos.csv")
# e = fread("e-pos.csv")
# e = as.matrix(e)
# p$`Acq. Date-Time`  = gsub("/13 ", "/2013 ", p$`Acq. Date-Time`)
# p$`Acq. Date-Time` = as.numeric(strptime(p$`Acq. Date-Time`, "%m/%d/%Y %H:%M"))
#
#
# path = "C:\\Users\\Sili Fan\\Desktop\\WORK\\WCMC\\projects\\mayo_depression_SSRIs_2013_normalization\\mayo_depression_SSRIs_2013_with_time_stamp.xlsx"
# for xlsx input
data = readData(path)
p = data$p
p$`Acq. Date-Time` = p$time
p$`Stat Level 1` = p$type
p$`Stat Level 1`[p$`Stat Level 1`=='validate'] = "NIST"
f = data$f
e = as.matrix(data$e)
if(sum(is.na(e)) > 0){
cat(paste0("NOTE: ",sum(is.na(e)), " missing values detected in the data. They will be replaced by the half-minimum for each compound."))
missing_compounds = which(is.na(e), arr.ind = T)[,1]
for(i in missing_compounds){
e[i, is.na(e[i,])] = 1/2 * min(e[i,!is.na(e[i,])])
}
}
e = data.matrix(e)
# ggplot2 theme
library(ggplot2)
theme.scatter = theme(
plot.title = element_text(size = rel(2), hjust = 0.5,face = 'bold',family = "Arial"),#title size.
# axis.title = element_text(size = rel(2)),
axis.text	 = element_text(colour = 'black'),
panel.background = element_blank(),
plot.background = element_blank(),
legend.key = element_rect(fill = "white",colour = "white"),
legend.title = element_text(face = 'bold'),
text=element_text(family="Arial")
)
batch = p$batch
batch = matrix(rep(batch,nrow(f)), nrow = nrow(f), byrow = T, ncol = nrow(p))
NISTavailable = sum(p$type=="NIST") > 0
cat(paste0("validate samples are not detected."))
# set parallel computing if necessary.
cat("Initializing the parallel procedure, using all cores.\n")
cl = makeCluster(detectCores())
# results will be saved in the result_norm.
result_norm = list()
cat("set up Monte Carlo cross-validation index. 5-fold 8/2 split.\n");
QC.index.train = QC.index.test = list()
n_CV = 5
seed = 8
set.seed(seed)
for(j in 1:n_CV){
QC.index = which(p$`Stat Level 1` == "QC")
QC.index.train.temp = sample(QC.index,round(length(QC.index)*.8))
QC.index.test.temp = QC.index[!QC.index%in%QC.index.train.temp]
QC.index.train. = rep(F,ncol(e))
QC.index.test. = rep(F,ncol(e))
QC.index.train.[QC.index.train.temp] = T
QC.index.test.[QC.index.test.temp] = T
QC.index.train[[j]] = QC.index.train.
QC.index.test[[j]] = QC.index.test.
}
cat("\n<========== Normalizations Started! ==========>\n");
# no normalization.
cat("\n<========== No Normalization Started! ==========>\n")
result_norm[['none']] = (none_norm(e=e,f=f,p=p))
none.QC.CV = RSD(result_norm[['none']]$e[,p$`Stat Level 1`=='QC'],f,p[p$`Stat Level 1`=='QC',],cl=cl)
none.validate = RSD(result_norm[['none']]$e[,p$`Stat Level 1`=='NIST'],f,p,cl=cl)
cat(paste0("No normalization QC CV RSD is ", signif(median(none.QC.CV), 4)*100,"%. ", signif(sum(none.QC.CV<0.10)/nrow(f),4)*100,"% of QC CV RSD < 10%.\n"))
if(NISTavailable){
cat( "No normalization validate QC RSD is ", signif(median(none.validate), 4)*100,"%. ", paste0(signif(sum(none.validate<0.10)/nrow(f),4)*100,"% of validate QC RSD < 10%.\n"))
}
dir.create("normalized-data-sets")
dta = data$original
dta[5:nrow(dta),3:ncol(dta)] = result_norm[['none']]$e
write.csv(dta, file="normalized-data-sets\\normalization-result-none-normalization.csv",row.names=FALSE)
# mTIC normalization.
cat("\n<========== mTIC Normalization Started! ==========>\n")
result_norm[['mTIC']] = (mTIC_norm(e=e,f=f,p=p))
mTIC.QC.CV = RSD(result_norm[['mTIC']]$e[,p$`Stat Level 1`=='QC'],f,p[p$`Stat Level 1`=='QC',],cl=cl)
mTIC.validate = RSD(result_norm[['mTIC']]$e[,p$`Stat Level 1`=='NIST'],f,p,cl=cl)
cat(paste0("mTIC normalization QC CV RSD is ", signif(median(mTIC.QC.CV), 4)*100,"%. ", signif(sum(mTIC.QC.CV<0.10)/nrow(f),4)*100,"% of QC CV RSD < 10%.\n"))
if(NISTavailable){
cat("mTIC normalization validate QC RSD is ", signif(median(mTIC.validate), 4)*100,"%. ", paste0(signif(sum(mTIC.validate<0.10)/nrow(f),4)*100,"% of validate QC RSD < 10%.\n"))
}
dta = data$original
dta[5:nrow(dta),3:ncol(dta)] = result_norm[['mTIC']]$e
write.csv(dta, file="normalized-data-sets\\normalization-result-mTIC-normalization.csv",row.names=FALSE)
getwd()
# workingdirectory is in which folder your file is.
workingdirectory = "C:\\Users\\Sili Fan\\Downloads\\LOCAL"
# filename is the name of your file. Check out the example.xlsx for format requirements.
filename = "mx 370132 Anne Flennicken, mouse plasma, September 2017.xlsx"
source(paste0(workingdirectory,"\\core.R"))
cat("\n<========== Batch-wise LOESS Normalization Started! ==========>\n")
result_norm[['loess']] = loess_norm(e = e, f=f, p=p,batch,QC.index,"Acq. Date-Time",span.para = 0.75)
loess.validate = RSD(result_norm[['loess']]$e[,p$`Stat Level 1`=="NIST"],f,p[p$`Stat Level 1`=="NIST",],cl=cl)
result_norm_loess_CV = list()
loess.QC.CV.  = list()
for(j in 1:n_CV){
time = p$`Acq. Date-Time`
qc. = QC.index.train[[j]]
norms = parSapply(cl, X = 1:nrow(f), function(i,eData,qc,batch,time,remove_outlier,span_para,get_loess_para,loess.span.limit){
models = by(data.frame(v=eData[i,qc],t=time[qc]),
batch[i,qc],function(x){
# x = data.frame(v=e[i,qc],t=time[qc])[batch[i,qc]=="B",]
if(length(remove_outlier(x$v)[[2]])>0){# if outlier exists.
span = ifelse(span_para=='auto',
get_loess_para(x=x$t[-remove_outlier(x$v)[[2]]],y=remove_outlier(x$v)[[1]],
loess.span.limit = loess.span.limit),span_para) # find a proper span.
}else{
span = ifelse(span_para=='auto',
get_loess_para(x=x$t,y=x$v,
loess.span.limit = loess.span.limit),span_para) # find a proper span.
}
if(length(remove_outlier(x$v)[[2]])>0){
tryCatch(loess(v~t,data=x[-remove_outlier(x$v)[[2]],],span=span),error = function(e){
NA
})
}else{
tryCatch(loess(v~t,data=x,span=span), error = function(e){
NA
})
}
})
# predict using the models.
norm = mapply(function(u,v){
o = tryCatch({
predict(u,newdata = v)
},
error = function(e){
print(e)
rep(0,length(v))
})
},models,by(time,batch[i,],function(x){x}))
norm = unlist(norm)
# replace NA with the closest value.
if(length(which(is.na(norm)))>0){
for(j in which(is.na(norm))){
NA_batch = batch[1,][j]
time_notNA = time[batch[1,]%in%NA_batch][-which(is.na(norm[batch[1,]%in%NA_batch]))]
closest_time = time_notNA[which.min(abs(time_notNA-time[j]))]
norm[j] = norm[batch[1,]%in%NA_batch][which(time[batch[1,]%in%NA_batch]==closest_time)[1]]
}
}
return(norm)
},e,qc.,batch,time,remove_outlier,0.75,get_loess_para,0.1)
norms = t(norms)
e_norm = matrix(NA,nrow=nrow(e),ncol=ncol(e))
# if(divide){
for(k in 1:nrow(e)){
e_norm[k,] = e[k,]/(norms[k,]/median(e[k,],na.rm = T))
}
result = e_norm
loess.QC.CV.[[j]] = RSD(result[,QC.index.test[[j]]],f,p[QC.index.test[[j]],],cl=cl)
}
loess.QC.CV = apply(do.call("cbind",loess.QC.CV.),1,mean, na.rm=T)
cat(paste0("loess normalization QC CV RSD is ", signif(median(loess.QC.CV, na.rm = T), 4)*100,"%. ", signif(sum(loess.QC.CV<0.10, na.rm = T)/nrow(f),4)*100,"% of QC CV RSD < 10%.\n"))
if(NISTavailable){
cat("loess normalization validate QC RSD is ", signif(median(loess.validate, na.rm = T), 4)*100,"%. ", paste0(signif(sum(loess.validate<0.10, na.rm = T)/nrow(f),4)*100,"% of validate QC RSD < 10%.\n"))
}
dta = data$original
dta[5:nrow(dta),3:ncol(dta)] = result_norm[['loess']]$e
write.csv(dta, file="normalized-data-sets\\normalization-result-loess-normalization.csv",row.names=FALSE)
cat("\n<========== Batch-wise LOESS Normalization Started! ==========>\n")
result_norm[['loess']] = loess_norm(e = e, f=f, p=p,batch,QC.index,"Acq. Date-Time",span.para = 0.75)
batch
QC.index
time
time = "Acq. Date-Time",
span.para = 'auto'
loess.span.limit=0.25)
loess.span.limit=0.25
e = data.matrix(e)
norms = parSapply(cl, X = 1:nrow(e), function(i,e,f,p,QC.index,batch,time,remove_outlier,span.para,get_loess_para,
loess.span.limit){
# for(i in 1:nrow(e)){
models = by(data.frame(v=e[i,QC.index],t=p[[time]][QC.index]),
factor(batch[i,QC.index]),function(x){
# x = data.frame(v=e[i,QC.index],t=p[[time]][QC.index])[batch[i,QC.index]=="A",]
if(length(remove_outlier(x$v)[[2]])>0){# if outlier exists.
span = ifelse(span.para=='auto',
get_loess_para(x=x$t[-remove_outlier(x$v)[[2]]],y=remove_outlier(x$v)[[1]],
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}else{
span = ifelse(span.para=='auto',
get_loess_para(x=x$t,y=x$v,
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}
if(length(remove_outlier(x$v)[[2]])>0){
loess(v~t,data=x[-remove_outlier(x$v)[[2]],],span=span)
}else{
loess(v~t,data=x,span=span)
}
})
# }
# predict using the models.
norm = mapply(function(u,v){
# o = tryCatch({
predict(u,newdata = v)
# },
# error = function(e){
# print(e)
# v
# })
},models,by(p[[time]],batch[i,],function(x){x}))
norm = unlist(norm)
# replace NA with the closest value.
if(length(which(is.na(norm)))>0){
for(j in which(is.na(norm))){
time_notNA = p[[time]][-which(is.na(norm))]
closest_time = time_notNA[which.min(abs(time_notNA - p[[time]][j]))]
norm[j] = norm[which(p[[time]]==closest_time)]
}
}
return(norm)
},e,f,p,QC.index,batch,time,remove_outlier,span.para,get_loess_para,loess.span.limit)
for(i in 1:nrow(e)){
models = by(data.frame(v=e[i,QC.index],t=p[[time]][QC.index]),
factor(batch[i,QC.index]),function(x){
# x = data.frame(v=e[i,QC.index],t=p[[time]][QC.index])[batch[i,QC.index]=="A",]
if(length(remove_outlier(x$v)[[2]])>0){# if outlier exists.
span = ifelse(span.para=='auto',
get_loess_para(x=x$t[-remove_outlier(x$v)[[2]]],y=remove_outlier(x$v)[[1]],
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}else{
span = ifelse(span.para=='auto',
get_loess_para(x=x$t,y=x$v,
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}
if(length(remove_outlier(x$v)[[2]])>0){
loess(v~t,data=x[-remove_outlier(x$v)[[2]],],span=span)
}else{
loess(v~t,data=x,span=span)
}
})
}
i
QC.index
p[[time]][QC.index]
time
time = "Acq. Date-Time"
for(i in 1:nrow(e)){
models = by(data.frame(v=e[i,QC.index],t=p[[time]][QC.index]),
factor(batch[i,QC.index]),function(x){
# x = data.frame(v=e[i,QC.index],t=p[[time]][QC.index])[batch[i,QC.index]=="A",]
if(length(remove_outlier(x$v)[[2]])>0){# if outlier exists.
span = ifelse(span.para=='auto',
get_loess_para(x=x$t[-remove_outlier(x$v)[[2]]],y=remove_outlier(x$v)[[1]],
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}else{
span = ifelse(span.para=='auto',
get_loess_para(x=x$t,y=x$v,
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}
if(length(remove_outlier(x$v)[[2]])>0){
loess(v~t,data=x[-remove_outlier(x$v)[[2]],],span=span)
}else{
loess(v~t,data=x,span=span)
}
})
}
i
span.para
span
x = data.frame(v=e[i,QC.index],t=p[[time]][QC.index])[batch[i,QC.index]=="A",]
if(length(remove_outlier(x$v)[[2]])>0){# if outlier exists.
span = ifelse(span.para=='auto',
get_loess_para(x=x$t[-remove_outlier(x$v)[[2]]],y=remove_outlier(x$v)[[1]],
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}else{
span = ifelse(span.para=='auto',
get_loess_para(x=x$t,y=x$v,
loess.span.limit = loess.span.limit),span.para) # find a proper span.
}
time
batch[i,QC.index]
View9batch
View(batch)
# workingdirectory is in which folder your file is.
workingdirectory = "C:\\Users\\Sili Fan\\Downloads\\LOCAL"
# filename is the name of your file. Check out the example.xlsx for format requirements.
filename = "mx 370132 Anne Flennicken, mouse plasma, September 2017.xlsx"
source(paste0(workingdirectory,"\\core.R"))
# workingdirectory is in which folder your file is.
workingdirectory = "C:\\Users\\Sili Fan\\Downloads\\LOCAL"
# filename is the name of your file. Check out the example.xlsx for format requirements.
filename = "mx 370132 Anne Flennicken, mouse plasma, September 2017.xlsx"
source(paste0(workingdirectory,"\\core.R"))
SERRF_norm
SERRF_norm = function(e,f,p,
batch = define_batch(e,f,p),
QC.index, time = "Acq. Date-Time"){
qc = rep(F, nrow(p))
qc[QC.index] = T
e. = e
diff = c()
# for(i in 1:nrow(e)){ # MAKE SURE THE QC AND SAMPLES ARE AT THE SAME LEVEL. This is critical for SERRF algorithm (and other tree-based machine learning algorithm) because when building each tree, the split on each leaf considers the level of the values. If the values are not consistant, then the RF models will be wrong and the RF will bias the intensity level after normalization (although the relative position won't change.)
#
#   e.[i,qc] = unlist(by(data.frame(e.[i,],qc),batch[1,],function(x){# x = data.frame(e.[i,],qc)[batch[1,]=='A',]
#     diff[i] <<- (median(x[x[,2],1]) - median(x[!x[,2],1]))
#     x[x[,2],1] - diff[i]
#   }))
# }
pred = parSapply(cl, X = 1:nrow(f), function(j,eData,batch,randomForest, QC.index, time){
data = data.frame(y = eData[j,], t(eData[-j,]), batch = batch[1,], time = time)
colnames(data) = c("y", paste0("X",1:nrow(eData))[-j], "batch", "time")
model = randomForest(y~., data = data,subset = QC.index, importance = F)
newdata = data.frame(t(eData[-j,]), batch = batch[1,], time = time)
colnames(newdata) =   c(paste0("X",1:nrow(eData))[-j], "batch", "time")
new = (eData[j,]/predict(model,newdata = newdata))*median(eData[j,])
return(new)
}, e.,batch,randomForest, QC.index, p[[time]])
e_SERRF_pred = t(pred)
# # put the QC level bach to where they were.
# for(i in 1:nrow(e_SERRF_pred)){
#   e_SERRF_pred[i,qc] = e_SERRF_pred[i,qc] + diff[i]
# }
return(list(e = e_SERRF_pred, p = p, f = f))
}
result_norm[['SERRF']] = SERRF_norm(e., f, p, batch, QC.index, time = "Acq. Date-Time")
SERRF.validate = RSD(result_norm[['SERRF']]$e[,p$`Stat Level 1`=="NIST"],f,p[p$`Stat Level 1`=="NIST",],cl=cl)
SERRF.validate
cat("Drawing PCA plot for each of the normalization method.")
# Generate PCA for each method.
doc = pptx( )
for(method in names(result_norm)){
if(sum(is.na(result_norm[[method]]))>0){
}else{
doc = addSlide(doc, slide.layout = "Title and Content")
index1 = apply(result_norm[[method]]$e, 1, function(x){
!sd(x, na.rm = T) == 0
})
index2 = !is.na(index1)
index = index1 & index2
doc = addPlot(doc, fun = function() {
print(generate_PCA(result_norm[[method]]$e[index,],f[index,],p,batch = batch[index,],QC.index =  QC.index,method))
},
vector.graphic = TRUE, width = 6, height = 6)
}
}
# write the document to a file
writeDoc(doc, file = "PCA score plots.pptx")
SERRF_norm = function(e,f,p,
batch = define_batch(e,f,p),
QC.index, time = "Acq. Date-Time"){
qc = rep(F, nrow(p))
qc[QC.index] = T
e. = e
diff = c()
# for(i in 1:nrow(e)){ # MAKE SURE THE QC AND SAMPLES ARE AT THE SAME LEVEL. This is critical for SERRF algorithm (and other tree-based machine learning algorithm) because when building each tree, the split on each leaf considers the level of the values. If the values are not consistant, then the RF models will be wrong and the RF will bias the intensity level after normalization (although the relative position won't change.)
#
#   e.[i,qc] = unlist(by(data.frame(e.[i,],qc),batch[1,],function(x){# x = data.frame(e.[i,],qc)[batch[1,]=='A',]
#     diff[i] <<- (median(x[x[,2],1]) - median(x[!x[,2],1]))
#     x[x[,2],1] - diff[i]
#   }))
# }
pred = parSapply(cl, X = 1:nrow(f), function(j,eData,batch,randomForest, QC.index, time){
data = data.frame(y = eData[j,], t(eData[-j,]), batch = batch[1,], time = time)
colnames(data) = c("y", paste0("X",1:nrow(eData))[-j], "batch", "time")
model = randomForest(y~., data = data,subset = QC.index, importance = F)
newdata = data.frame(t(eData[-j,]), batch = batch[1,], time = time)
colnames(newdata) =   c(paste0("X",1:nrow(eData))[-j], "batch", "time")
new = (eData[j,]/predict(model,newdata = newdata))*median(eData[j,])
return(new)
}, e.,batch,randomForest, QC.index, p[[time]])
e_SERRF_pred = t(pred)
# # put the QC level bach to where they were.
# for(i in 1:nrow(e_SERRF_pred)){
#   e_SERRF_pred[i,qc] = e_SERRF_pred[i,qc] + diff[i]
# }
return(list(e = e_SERRF_pred, p = p, f = f))
}
cat("\n<========== SERRF Normalization Started! ==========>\n")
cat("This may take some time. \n")
qc = rep(F, nrow(p))
qc[QC.index] = T
e. = e
# for(i in 1:nrow(e)){ # MAKE SURE THE QC AND SAMPLES ARE AT THE SAME LEVEL. This is critical for SERRF algorithm (and other tree-based machine learning algorithm) because when building each tree, the split on each leaf considers the level of the values. If the values are not consistant, then the RF models will be wrong and the RF will bias the intensity level after normalization (although the relative position won't change.)
#   e.[i,qc] = unlist(by(data.frame(e.[i,],qc),batch[1,],function(x){# x = data.frame(e.[i,],qc)[batch[1,]=='A',]
#     x[x[,2],1] - (median(x[x[,2],1]) - median(x[!x[,2],1]))
#   }))
# }
result_norm[['SERRF']] = SERRF_norm(e., f, p, batch, QC.index, time = "Acq. Date-Time")
SERRF.validate = RSD(result_norm[['SERRF']]$e[,p$`Stat Level 1`=="NIST"],f,p[p$`Stat Level 1`=="NIST",],cl=cl)
result_norm_SERRF_CV = list()
SERRF.QC.CV.  = list()
for(i in 1:n_CV){
time = p$`Acq. Date-Time`
qc. = QC.index.train[[i]]
e_SERRF_pred = parSapply(cl, X = 1:nrow(f), function(j,eData,batch,randomForest, qc., time){
data = data.frame(y = eData[j,], t(eData[-j,]), batch = batch[1,], time = time)
colnames(data) = c("y", paste0("X",1:nrow(eData))[-j], "batch", "time")
model = randomForest(y~., data = data,subset = qc., importance = F, ntree = 500)
newdata = data.frame(t(eData[-j,]), batch = batch[1,], time = time)
colnames(newdata) =   c(paste0("X",1:nrow(eData))[-j], "batch", "time")
new = (eData[j,]/predict(model,newdata = newdata)) * median(eData[j,])
return(new)
}, e.,batch,randomForest, qc., p$`Acq. Date-Time`)
e_SERRF_pred = t(e_SERRF_pred)
dta = e_SERRF_pred[,QC.index.test[[i]]]
# dta = mTIC_norm(e=dta,f=f,p=p[QC.index.test[[i]],])$e
SERRF.QC.CV.[[i]] = RSD(dta,f,p[QC.index.test[[i]],],cl=cl)
}
SERRF.QC.CV = apply(do.call("cbind",SERRF.QC.CV.),1,mean, na.rm=T)
cat(paste0("SERRF normalization QC CV RSD is ", signif(median(SERRF.QC.CV, na.rm = T), 4)*100,"%. ",signif(sum(SERRF.QC.CV<0.10, na.rm = T)/nrow(f),4)*100,"% of QC CV RSD < 10%.\n"))
if(NISTavailable){
cat("SERRF normalization validate QC RSD is ", signif(median(SERRF.validate, na.rm = T), 4)*100,"%. ",signif(sum(SERRF.validate<0.10, na.rm = T)/nrow(f),4)*100,"% of validate QC RSD < 10%.\n")
}
dta = data$original
dta[5:nrow(dta),3:ncol(dta)] = result_norm[['SERRF']]$e
write.csv(dta, file="normalized-data-sets\\normalization-result-SERRF-normalization.csv",row.names=FALSE)
cat("Drawing PCA plot for each of the normalization method.")
# Generate PCA for each method.
doc = pptx( )
for(method in names(result_norm)){
if(sum(is.na(result_norm[[method]]))>0){
}else{
doc = addSlide(doc, slide.layout = "Title and Content")
index1 = apply(result_norm[[method]]$e, 1, function(x){
!sd(x, na.rm = T) == 0
})
index2 = !is.na(index1)
index = index1 & index2
doc = addPlot(doc, fun = function() {
print(generate_PCA(result_norm[[method]]$e[index,],f[index,],p,batch = batch[index,],QC.index =  QC.index,method))
},
vector.graphic = TRUE, width = 6, height = 6)
}
}
# write the document to a file
writeDoc(doc, file = "PCA score plots.pptx")
# workingdirectory is in which folder your file is.
workingdirectory = "C:\\Users\\Sili Fan\\Downloads\\LOCAL"
# filename is the name of your file. Check out the example.xlsx for format requirements.
filename = "mx 370132 Anne Flennicken, mouse plasma, September 2017.xlsx"
source(paste0(workingdirectory,"\\core.R"))
