{
    "collab_server" : "",
    "contents" : "SERRF = function(input = \"C:\\\\Users\\\\Sili Fan\\\\Downloads\\\\Constanze GCTOF for Normalization.xlsx\", ip = '128.120.143.234'){\n\n  library(rgeolocate)\n\n  start = Sys.time()\n\n  readData = function(path =  \"G:\\\\data\\\\D\\\\data project D.xlsx\"){\n\n    #check if it is csv of xlsx\n    if(grepl(\"xlsx\", path)){\n      d <- openxlsx::read.xlsx(path, sheet = 1,colNames = FALSE)\n    }else if(grepl(\"csv\", path)){\n      # file = \"C:\\\\Users\\\\Sili Fan\\\\Downloads\\\\val (18).csv\"\n      d <- data.table::fread(path)\n    }\n\n    # make \"\" as NA\n    d[d==\"\"] <- NA\n\n    #### fData\n    fData <- d[!is.na(d[,1]),c(which(is.na(d[1,])),sum(is.na(d[1,]))+1)] # The first row and column is critical of formating the data.\n    colnames(fData) = as.character(fData[1,]); fData = data.frame(fData[-1,],stringsAsFactors = F,check.names = FALSE);rownames(fData) = 1:nrow(fData);\n    # following steps keeps the column type.\n    fData.=lapply(fData,function(x){\n      if(sum(!is.na(as.numeric(x))) == length(x)){\n        as.numeric(x)\n      }else{\n        x\n      }\n    })\n    fData. = do.call(cbind, lapply(fData., data.frame, stringsAsFactors=FALSE))\n    colnames(fData.) = colnames(fData)\n    fData = fData.\n\n    fData = fData[,c(ncol(fData),2:ncol(fData)-1)]\n    fData[[1]] = make.unique(fData[[1]], sep = '_')\n\n    #### pData\n    pData <- d[c(which(is.na(d[,1])),max(which(is.na(d[,1])))+1) ,!is.na(d[1,])]\n    pData <- t(pData); colnames(pData) = pData[1,]; pData = data.frame(pData[-1,],stringsAsFactors = F,check.names = FALSE)\n    # following steps keeps the column type.\n    pData.=lapply(pData,function(x){\n      if(sum(!is.na(as.numeric(x))) == length(x)){\n        as.numeric(x)\n      }else{\n        x\n      }\n    })\n    pData. = do.call(cbind, lapply(pData., data.frame, stringsAsFactors=FALSE))\n    colnames(pData.) = colnames(pData)\n    pData = pData.\n\n    pData = pData[,c(ncol(pData),2:ncol(pData)-1)]\n    pData[[1]] = make.unique(make.names(pData[[1]]), sep = '_')\n\n    #### eData\n    eData <- d[!is.na(d[,1]),!is.na(d[1,])][-1,-1]\n    eData <- sapply(eData, as.numeric)\n    eData <- data.frame(eData,stringsAsFactors = F)\n    colnames(eData) = pData[[1]]; rownames(eData) = fData[[1]]\n\n    # # remove any unwanted character in columns of eData, fData and pData to _.\n    # colnames(eData) = gsub(\"([_])|[[:punct:]]\", \"_\", colnames(eData))\n    # colnames(fData) = gsub(\"([_])|[[:punct:]]\", \"_\", colnames(fData))\n    # colnames(pData) = gsub(\"([_])|[[:punct:]]\", \"_\", colnames(pData))\n\n    # remove all the NA. And replace NA with \"NA\" Otherwise DataTables will give error.datatables warning requested unknown parameter\n    # eData[is.na(eData)]=\"NA\"\n    # fData[is.na(fData)]=\"NA\"\n    # pData[is.na(pData)]=\"NA\"\n\n    # remove unwanted character in p.\n    # for(i in 1:nrow(pData)){\n    #   for(j in 1:ncol(pData)){\n    #     pData[i,j] = gsub(\"\\\\+|~|-\", \" \", pData[i,j])\n    #   }\n    # }\n\n    return(list(e = eData, f = fData, p = pData))\n\n  }\n\n  info = read.csv(paste0(\"http://localhost:5984/serrf/info/info.csv\"), stringsAsFactors = F,na.strings = \"\")\n\n\n  file <- system.file(\"extdata\",\"ip2_sample.bin\", package = \"rgeolocate\")\n\n  info$num[info$code == ip2location(ip, file, c(\"country_code\"))[[1]]] =   info$num[info$code == ip2location(ip, file, c(\"country_code\"))[[1]]]+1\n\n  put_att_csv = function(projectID = 'tryThu.Aug.17.14.53.35.2017', attname = 'test.csv', att = data.table::fread(\"G:\\\\initialize MetDA\\\\user_active.csv\")){\n    projectUrl <- paste0(\"http://localhost:5984/serrf/\",projectID)\n    projectList <- jsonlite::fromJSON(projectUrl)\n\n    new_att = projectList[[\"_attachments\"]]\n    new_att = new_att[!names(new_att)%in%attname]\n    new_att[[attname]] = list(content_type=\"text/csv\", data = RCurl::base64(\n      paste0(R.utils::captureOutput(write.csv(att,stdout(), row.names=F)),collapse = \"\\n\")\n    ))\n    projectList[[\"_attachments\"]] = new_att\n    result = RCurl::getURL(paste0(\"http://localhost:5984/serrf/\",projectID),customrequest='PUT',httpheader=c('Content-Type'='application/json'),postfields= jsonlite::toJSON(projectList,auto_unbox = T, force = T))\n    while(grepl(\"error\",result)) {\n      projectList <- jsonlite::fromJSON(projectUrl)\n      new_att = projectList[[\"_attachments\"]]\n      new_att = new_att[!names(new_att)%in%attname]\n      new_att[[attname]] = list(content_type=\"text/csv\", data = RCurl::base64(\n        paste0(R.utils::captureOutput(write.csv(att,stdout(), row.names=F)),collapse = \"\\n\")\n      ))\n      projectList[[\"_attachments\"]] = new_att\n      result = RCurl::getURL(paste0(\"http://localhost:5984/serrf/\",projectID),customrequest='PUT',httpheader=c('Content-Type'='application/json'),postfields= jsonlite::toJSON(projectList,auto_unbox = T, force = T))\n      if(grepl(\"ok\",result)){\n        break;\n      }\n    }\n    return(result)\n  }\n\n  put_att_csv(\"info\", \"info.csv\", info)\n\n  data = readData(input)\n\n  e = data$e\n  f = data$f\n  p = data$p\n\n  # impute missing value.\n  if(sum(is.na(e))>0){\n    missing_compounds = which(is.na(e), arr.ind = T)[,1]\n    for(i in missing_compounds){\n      e[i, is.na(e[i,])] = 1/2 * min(e[i,!is.na(e[i,])])\n    }\n  }\n  e = data.matrix(e)\n\n\n  library(ggplot2)\n  theme.scatter = theme(\n    plot.title = element_text(size = rel(2), hjust = 0.5,face = 'bold',family = \"Arial\"),#title size.\n    # axis.title = element_text(size = rel(2)),\n    axis.text\t = element_text(colour = 'black'),\n    panel.background = element_blank(),\n    plot.background = element_blank(),\n    legend.key = element_rect(fill = \"white\",colour = \"white\"),\n    legend.title = element_text(face = 'bold'),\n    text=element_text(family=\"Arial\")\n  )\n\n  # define batch.\n  batch = matrix(rep(p$batch, nrow(f)), nrow = nrow(f), byrow = T)\n  QC.index = which(p$type == \"QC\")\n\n  # parallel computing.\n  library(parallel)\n  cl = makeCluster(8)\n\n  # SERRF normalization\n  SERRF_norm = function(e,f,p,\n                        batch = define_batch(e,f,p),\n                        QC.index, time = \"Acq. Date-Time\"){\n    library(randomForest)\n    qc = rep(F, nrow(p))\n    qc[QC.index] = T\n    e. = e\n    for(i in 1:nrow(e)){ # MAKE SURE THE QC AND SAMPLES ARE AT THE SAME LEVEL. This is critical for SERRF algorithm (and other tree-based machine learning algorithm) because when building each tree, the split on each leaf considers the level of the values. If the values are not consistant, then the RF models will be wrong and the RF will bias the intensity level after normalization (although the relative position won't change.)\n      e.[i,qc] = unlist(by(data.frame(e.[i,],qc),batch[1,],function(x){# x = data.frame(e.[i,],qc)[batch[1,]=='A',]\n        x[x[,2],1] - (median(x[x[,2],1]) - median(x[!x[,2],1]))\n      }))\n    }\n    pred = parSapply(cl, X = 1:nrow(f), function(j,eData,batch,randomForest, QC.index, time){\n      data = data.frame(y = eData[j,], t(eData[-j,]), batch = batch[1,], time = time)\n      colnames(data) = c(\"y\", paste0(\"X\",1:nrow(eData))[-j], \"batch\", \"time\")\n      model = randomForest(y~., data = data,subset = QC.index, importance = F)\n      newdata = data.frame(t(eData[-j,]), batch = batch[1,], time = time)\n      colnames(newdata) =   c(paste0(\"X\",1:nrow(eData))[-j], \"batch\", \"time\")\n      new = (eData[j,]/predict(model,newdata = newdata))*median(eData[j,])\n      return(new)\n    }, e.,batch,randomForest, QC.index, p[[time]])\n\n    e_SERRF_pred = t(pred)\n    return(list(e = e_SERRF_pred, p = p, f = f))\n  }\n\n  norm = SERRF_norm(e, f, p, batch, QC.index, time = \"time\")\n\n\n\n  # evaluation methods.\n  # RSD\n  RSD = function(e,f,p,robust = F,cl){\n    library(parallel)\n    if(robust){\n      result=parSapply(cl=cl,X=1:nrow(e),FUN = function(i,remove_outlier,e){\n        x = remove_outlier(e[i,])[[1]]\n        sd(x,na.rm=T)/mean(x,na.rm=T)\n      },remove_outlier,e)\n    }else{\n      result=parSapply(cl=cl,X=1:nrow(e),FUN = function(i,e){\n        x = e[i,]\n        sd(x,na.rm=T)/mean(x,na.rm=T)\n      },e)\n    }\n\n\n    return(result)\n  }\n  SERRF.validate = RSD(norm$e[,p$type==\"validate\"],f,p[p$type==\"validate\",],cl=cl)\n  raw.validate = RSD(e[,p$type==\"validate\"],f,p[p$type==\"validate\",],cl=cl)\n  stopCluster(cl)\n  # PCA\n  # generate PCA plot.\n  generate_PCA = function(e, f, p, QC.index, batch, method){\n    pca = prcomp(t(e), center = T, scale. = T)\n    variance = pca$sdev^2/sum(pca$sdev^2)\n    pca.data = data.frame(pca$x,batch = batch[1,],order = 1:nrow(pca$x))\n    batch.QC = batch[1,];\n    batch.QC[QC.index] = \"QC\"\n    qc = rep(F, nrow(p))\n    qc[QC.index] = TRUE\n    ggplot(pca.data, aes(PC1, PC2, color = batch.QC,size = qc, order = order)) +\n      geom_point(alpha = 3/4) +\n      stat_ellipse( linetype = 2, size = 0.5) +\n      labs(x = paste0(\"PC1: \",signif(variance[1]*100,3),\"%\"), y = paste0(\"PC2: \",signif(variance[2]*100,3),\"%\"),\n           title = method)+\n      theme.scatter\n  }\n\n  SERRFpca = generate_PCA(norm$e,f,p,QC.index, batch , \"SERRF\")\n  rawpca = generate_PCA(e,f,p,QC.index, batch , \"raw\")\n\n\n  # save results.\n  rownames(norm$e) = f$label\n  write.csv(norm$e, \"SERRF_normalized.csv\")\n  write.csv(data.frame(label=f$label,rawValidateRSD = raw.validate, SERRFValidateRSD = SERRF.validate), 'performance -  validateRSD.csv')\n  ggsave(\"SERRFpca.png\",SERRFpca, width = 8, height = 8)\n  ggsave(\"rawpca.png\",rawpca, width = 8, height = 8)\n\n  library(ReporteRs)\n  doc = pptx( )\n  doc = addSlide(doc, slide.layout = \"Title and Content\")\n  doc = addPlot(doc, fun = function() print(rawpca),\n                vector.graphic = TRUE, width = 6, height = 6)\n  doc = addSlide(doc, slide.layout = \"Title and Content\")\n  doc = addPlot(doc, fun = function() print(SERRFpca),\n                vector.graphic = TRUE, width = 6, height = 6)\n\n  # write the document to a file\n  writeDoc(doc, file = \"PCAs.pptx\")\n\n  zip(files = c(\n    \"SERRF_normalized.csv\",\n    'performance -  validateRSD.csv',\n    \"SERRFpca.png\",\n    \"rawpca.png\",\n    \"PCAs.pptx\"\n  ), zipfile = \"SERRF - results.zip\")\n  end = Sys.time()\n\n\n\n\n\n\n\n  # get ip summary\n  ip_summ = t(info$num)\n  colnames(ip_summ) = info$code\n\n  ip_summ = data.frame(ip_summ)\n\n  ip_summ = jsonlite::toJSON(ip_summ)\n\n\n\n\n\n  return(list(validateSERRF = signif(median(SERRF.validate),3)*100, validateraw = signif(median(raw.validate),3)*100, count_reduced = sum(SERRF.validate<raw.validate, na.rm = T), perc_reduced = sum(SERRF.validate<raw.validate, na.rm = T)/nrow(f), count_less_20_raw = sum(raw.validate<.2, na.rm = T), count_less_20_SERRF = sum(SERRF.validate<.2, na.rm = T), perc_less_20_raw = signif(sum(raw.validate<.2, na.rm = T)/nrow(f),3) * 100, perc_less_20_SERRF = signif(sum(SERRF.validate<.2, na.rm = T)/nrow(f),3)*100, runtime = signif(as.numeric(end - start)/60,3),ip_summ=ip_summ))\n\n}\n",
    "created" : 1505759142925.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "6|66|80|2|\n89|156|114|2|\n",
    "hash" : "3459581318",
    "id" : "763AC25C",
    "lastKnownWriteTime" : 1505838500,
    "last_content_update" : 1505838500652,
    "path" : "~/GitHub/SERRFweb/R/SERRF.R",
    "project_path" : "R/SERRF.R",
    "properties" : {
        "source_window_id" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}